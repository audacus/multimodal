# Multimodal AI Agent with LangGraph & Telegram

A bi-directional multimodal AI agent that processes text, images, audio, and video through a Telegram bot interface. Built with LangChain/LangGraph v1.x, powered by Qwen3 Omni, and deployable on DigitalOcean GPU Droplets.

## Features

### ğŸ¯ Core Capabilities
- **Multimodal Processing**: Text, images, audio, and video understanding
- **Tool Usage**: Integration with MCP (Model Context Protocol) servers
- **Bi-directional Communication**: Real-time conversations via Telegram
- **State Management**: LangGraph-powered conversation flow
- **GPU Optimized**: Efficient inference on DigitalOcean GPU Droplets

### ğŸ› ï¸ Technology Stack
- **Backend**: LangChain/LangGraph v1.x
- **Frontend**: Telegram Bot (python-telegram-bot)
- **Model**: Qwen3 Omni (multimodal LLM)
- **Tools**: MCP servers for external integrations
- **Deployment**: Docker + NVIDIA GPU support

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Telegram Bot   â”‚
â”‚   (Frontend)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangGraph      â”‚
â”‚  Agent          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Agent    â”‚  â”‚
â”‚  â”‚  Node     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
â”‚        â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Tools    â”‚  â”‚
â”‚  â”‚  Node     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”
â”‚Qwen3â”‚   â”‚ MCP â”‚
â”‚Omni â”‚   â”‚Srvs â”‚
â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### 1. Prerequisites

- Python 3.10-3.13 (recommended: 3.13)
- NVIDIA GPU (for local deployment)
- Docker & Docker Compose
- Telegram Bot Token ([Create one with @BotFather](https://t.me/BotFather))

### 2. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd multimodal

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit configuration
nano .env
```

Required environment variables:
```bash
TELEGRAM_BOT_TOKEN=your_bot_token_here
MODEL_NAME=Qwen/Qwen3-Omni
MODEL_API_BASE=http://localhost:8000/v1  # Or your GPU droplet IP
```

### 4. Run Locally

```bash
# Start the bot
python main.py
```

## Deployment on DigitalOcean GPU Droplet

### Option 1: Automated Setup

```bash
# Run setup script
chmod +x deploy/digitalocean_setup.sh
./deploy/digitalocean_setup.sh

# Configure environment
nano .env

# Start with Docker Compose
docker-compose up -d --build

# View logs
docker-compose logs -f
```

### Option 2: Systemd Service

```bash
# Create systemd service
chmod +x deploy/systemd_service.sh
./deploy/systemd_service.sh

# Enable and start service
sudo systemctl enable multimodal-agent
sudo systemctl start multimodal-agent

# Check status
sudo systemctl status multimodal-agent
```

## MCP Server Configuration

Configure MCP servers in `config/mcp_servers.json`:

```json
{
  "servers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"],
      "transport": "stdio",
      "description": "File system operations"
    },
    "custom_api": {
      "transport": "streamable_http",
      "url": "http://localhost:8001/mcp",
      "headers": {
        "Authorization": "Bearer YOUR_TOKEN"
      },
      "description": "Custom API integration"
    }
  }
}
```

### Available Transport Types
- **stdio**: Local process communication
- **streamable_http**: HTTP-based (for remote servers)
- **sse**: Server-Sent Events

## Usage Examples

### Text Conversation
```
User: Explain quantum computing in simple terms
Bot: [Provides explanation using Qwen3 Omni]
```

### Image Analysis
```
User: [Sends photo] What's in this image?
Bot: [Analyzes and describes the image content]
```

### Audio Processing
```
User: [Sends audio file] Transcribe this
Bot: [Provides transcription using multimodal capabilities]
```

### Video Understanding
```
User: [Sends video] Summarize this video
Bot: [Analyzes and summarizes video content]
```

### Tool Usage
```
User: Search for recent papers on AI
Bot: [Uses web search MCP tool to find and summarize papers]
```

## Project Structure

```
multimodal/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          # LangGraph agent implementation
â”‚   â”‚   â”œâ”€â”€ graph.py    # Agent graph and state management
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ bot/            # Telegram bot frontend
â”‚   â”‚   â”œâ”€â”€ telegram_bot.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ mcp/            # MCP client integration
â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/         # Qwen3 Omni model wrapper
â”‚   â”‚   â”œâ”€â”€ qwen_omni.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ tools/          # Custom tools (optional)
â”‚   â”œâ”€â”€ utils/          # Utility functions
â”‚   â””â”€â”€ config.py       # Configuration management
â”œâ”€â”€ config/
â”‚   â””â”€â”€ mcp_servers.json # MCP server configuration
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ digitalocean_setup.sh
â”‚   â””â”€â”€ systemd_service.sh
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ logs/               # Application logs
â”œâ”€â”€ main.py             # Application entry point
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ Dockerfile          # Docker image definition
â”œâ”€â”€ docker-compose.yml  # Docker Compose configuration
â”œâ”€â”€ .env.example        # Environment variables template
â””â”€â”€ README.md           # This file
```

## Telegram Bot Commands

- `/start` - Start conversation and see welcome message
- `/help` - Show help and usage information
- `/clear` - Clear conversation history

## Development

### Running Tests

```bash
pytest tests/
```

### Code Formatting

```bash
black src/
ruff check src/
```

### Adding Custom Tools

Create a new tool in `src/tools/`:

```python
from langchain_core.tools import tool

@tool
def my_custom_tool(query: str) -> str:
    """Description of what this tool does."""
    # Implementation
    return result
```

Then add it to the agent:

```python
from src.tools.my_tool import my_custom_tool

agent = await create_agent(
    custom_tools=[my_custom_tool],
    use_mcp=True
)
```

## Monitoring & Logging

Logs are stored in the `logs/` directory:
- `app.log` - General application logs
- `telegram_bot.log` - Telegram bot specific logs

View live logs:
```bash
tail -f logs/app.log
```

With Docker:
```bash
docker-compose logs -f
```

## Performance Optimization

### GPU Memory Management
Adjust in `.env`:
```bash
TORCH_DTYPE=float16  # or bfloat16 for better quality
CUDA_VISIBLE_DEVICES=0
```

### Conversation History
Limit memory usage:
```bash
MAX_CONVERSATION_HISTORY=10  # Keep last 10 messages
```

### Model Caching
Models are cached in Docker volumes to avoid re-downloading.

## Troubleshooting

### GPU Not Detected
```bash
# Verify NVIDIA drivers
nvidia-smi

# Test Docker GPU access
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### Telegram Bot Not Responding
1. Check bot token in `.env`
2. Verify bot is running: `docker-compose ps`
3. Check logs: `docker-compose logs -f`

### MCP Tools Not Loading
1. Verify `config/mcp_servers.json` syntax
2. Check MCP server endpoints are accessible
3. Review logs for connection errors

### Out of Memory
- Reduce `MAX_TOKENS` in `.env`
- Use `TORCH_DTYPE=float16`
- Limit `MAX_CONVERSATION_HISTORY`

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

[Your License Here]

## Acknowledgments

Built with:
- [LangChain](https://github.com/langchain-ai/langchain)
- [LangGraph](https://github.com/langchain-ai/langgraph)
- [python-telegram-bot](https://github.com/python-telegram-bot/python-telegram-bot)
- [Qwen Models](https://github.com/QwenLM/Qwen)
- [Model Context Protocol](https://modelcontextprotocol.io/)

## Resources

### Documentation
- [LangGraph Multi-Agent Workflows](https://blog.langchain.com/langgraph-multi-agent-workflows/)
- [LangChain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters)
- [Build multimodal agents using Gemini, Langchain, and LangGraph](https://cloud.google.com/blog/products/ai-machine-learning/build-multimodal-agents-using-gemini-langchain-and-langgraph)
- [LangGraph MCP Integration Guide](https://latenode.com/blog/langgraph-mcp-integration-complete-model-context-protocol-setup-guide-working-examples-2025)

### Community
- [LangChain Discord](https://discord.gg/langchain)
- [Model Context Protocol](https://modelcontextprotocol.io/)

## Support

For issues and questions:
- GitHub Issues: [Your Repo Issues]
- Email: [Your Email]

---

**Note**: This project uses Qwen3 Omni. Ensure you comply with the model's license and usage terms.
